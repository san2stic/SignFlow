# TorchServe Multi-Device Setup

Configuration TorchServe pour supporter **CPU, MPS (Apple Silicon), et CUDA GPU** de mani√®re transparente.

## üöÄ D√©marrage Rapide

### Apple Silicon (M1/M2/M3) - MPS

```bash
# Build et d√©marrage
docker-compose -f docker-compose.yml -f docker-compose.arm64.yml up --build torchserve

# Le handler d√©tectera automatiquement MPS
```

### x86_64 avec NVIDIA GPU - CUDA

```bash
# Pr√©requis: NVIDIA Container Toolkit install√©
docker-compose -f docker-compose.yml -f docker-compose.gpu.yml up --build torchserve
```

### CPU uniquement (toutes plateformes)

```bash
docker-compose -f docker-compose.yml -f docker-compose.cpu.yml up --build torchserve
```

## üì¶ Architecture

```
backend/
‚îú‚îÄ‚îÄ Dockerfile.torchserve         # Build multi-platform
‚îú‚îÄ‚îÄ torchserve/
‚îÇ   ‚îú‚îÄ‚îÄ start.sh                  # D√©tection device auto
‚îÇ   ‚îú‚îÄ‚îÄ handlers/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sign_handler.py       # Handler multi-device
‚îÇ   ‚îú‚îÄ‚îÄ model-store/              # Mod√®les .mar
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îî‚îÄ‚îÄ config.properties     # Config TorchServe
```

## üîß D√©tection Automatique du Device

Le script `start.sh` d√©tecte automatiquement dans cet ordre :
1. **CUDA** : Si `torch.cuda.is_available()` ‚Üí utilise GPU NVIDIA
2. **MPS** : Si `torch.backends.mps.is_available()` ‚Üí utilise GPU Apple
3. **CPU** : Fallback si aucun GPU d√©tect√©

### Override manuel

```bash
# Forcer CPU
TORCH_DEVICE=cpu docker-compose up torchserve

# Forcer MPS
TORCH_DEVICE=mps docker-compose up torchserve

# Forcer CUDA
TORCH_DEVICE=cuda docker-compose up torchserve
```

## üìä Formats de Mod√®les Support√©s

### 1. PyTorch (.pt)
```bash
# TorchScript JIT
torch.jit.save(traced_model, "model.pt")

# Ou sauvegarde directe (n√©cessite architecture)
torch.save(model, "model.pt")
```

### 2. ONNX (.onnx)
```bash
# Export ONNX (recommand√© pour performance CPU)
python backend/app/ml/export.py --model-path model.pt --output-path model.onnx
```

**Providers ONNX selon device :**
- **CUDA** : `CUDAExecutionProvider` ‚Üí 2-3x plus rapide
- **CPU** : `CPUExecutionProvider` ‚Üí baseline
- **MPS** : Pas de provider natif ‚Üí fallback `CPUExecutionProvider`

## üèóÔ∏è Cr√©er un Model Archive (.mar)

```bash
# Depuis backend/
torch-model-archiver \
  --model-name signflow_model \
  --version 1.0 \
  --model-file app/ml/model.py \
  --serialized-file data/models/model.pt \
  --handler torchserve/handlers/sign_handler.py \
  --export-path torchserve/model-store \
  --extra-files "app/ml/feature_engineering.py,app/ml/model_configs.py" \
  --requirements-file requirements.txt
```

## üß™ Test de l'Inf√©rence

```bash
# Health check
curl http://localhost:8080/ping

# Lister les mod√®les
curl http://localhost:8081/models

# Inf√©rence
curl -X POST http://localhost:8080/predictions/signflow_model \
  -H "Content-Type: application/json" \
  -d '{
    "landmarks": [
      [[0.5, 0.5, 0.1], [0.6, 0.4, 0.15], ...],
      [[0.5, 0.6, 0.12], [0.61, 0.41, 0.16], ...]
    ]
  }'

# R√©ponse inclut le device utilis√©
{
  "predictions": [
    {"label": "hello", "confidence": 0.95}
  ],
  "device": "mps"  # ou "cuda", "cpu"
}
```

## üìà M√©triques Prometheus

```bash
# M√©triques d'inf√©rence
curl http://localhost:8082/metrics

# M√©triques cl√©s :
# - ts_inference_latency_microseconds : latence par device
# - ts_queue_latency_microseconds : temps d'attente batch
# - ts_inference_requests_total : nombre de requ√™tes
```

## ‚öôÔ∏è Configuration Performance

### Apple Silicon (MPS)
```yaml
# docker-compose.arm64.yml
deploy:
  resources:
    limits:
      memory: 6G    # MPS utilise unified memory
      cpus: '3.0'   # 3 cores suffisants
```

### NVIDIA GPU (CUDA)
```yaml
# docker-compose.gpu.yml
deploy:
  resources:
    limits:
      memory: 8G
      cpus: '4.0'
    reservations:
      devices:
        - driver: nvidia
          count: 1    # 1+ GPUs
```

### CPU uniquement
```yaml
# docker-compose.cpu.yml
deploy:
  resources:
    limits:
      memory: 4G
      cpus: '2.0'
```

## üêõ Troubleshooting

### "platform (linux/amd64) does not match (linux/arm64)"
‚Üí Utilisez `docker-compose.arm64.yml` ou `docker-compose.cpu.yml`

### MPS d√©tect√© mais pas utilis√©
```bash
# V√©rifier support MPS dans container
docker exec signflow_torchserve python3 -c "
import torch
print(f'MPS available: {torch.backends.mps.is_available()}')
print(f'MPS built: {torch.backends.mps.is_built()}')
"
```

### CUDA non d√©tect√©
```bash
# V√©rifier NVIDIA runtime
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# V√©rifier dans container
docker exec signflow_torchserve python3 -c "
import torch
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'CUDA devices: {torch.cuda.device_count()}')
"
```

### Latence √©lev√©e sur CPU
‚Üí Utilisez ONNX Runtime pour ~2-5x speedup :
```bash
python backend/app/ml/export.py --optimize
```

## üìö Performance Attendue

| Device | Latence (ms) | Throughput (req/s) | Notes |
|--------|--------------|-------------------|-------|
| **CUDA GPU** | 5-15 ms | 200-500 | Optimal pour production |
| **Apple MPS** | 10-30 ms | 100-200 | Bon pour d√©veloppement M1/M2 |
| **CPU (ONNX)** | 15-50 ms | 50-100 | Acceptable pour dev/test |
| **CPU (PyTorch)** | 40-120 ms | 20-40 | Fallback uniquement |

## üîó Int√©gration Backend FastAPI

Le backend d√©tecte automatiquement si TorchServe est actif :

```python
# backend/app/ml/pipeline.py
if USE_TORCHSERVE:
    response = requests.post(
        f"{TORCHSERVE_URL}/predictions/signflow_model",
        json={"landmarks": landmarks}
    )
    return response.json()
else:
    # Fallback: PyTorch direct
    return pytorch_inference(landmarks)
```

## üìù Prochaines √âtapes

1. **Batching Asynchrone** : Activer `batch_size > 1` dans `config.properties`
2. **Model Versioning** : A/B testing avec `canary_percentage`
3. **Drift Detection** : Monitoring de distribution via Prometheus
4. **Auto-scaling** : Kubernetes HPA sur m√©triques latence

---

**Support** : MPS (PyTorch 2.0+), CUDA 12.1+, CPU (x86_64/ARM64)
