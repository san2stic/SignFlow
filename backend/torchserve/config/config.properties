# Inference API
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082

# Workers
default_workers_per_model=2

# Batching
batch_size=16
max_batch_delay=50

# GPU
number_of_gpu=1
enable_envvars_config=true

# Logging
install_py_dep_per_model=true
enable_metrics_api=true

# Timeouts and limits
default_response_timeout=120
max_response_size=104857600
job_queue_size=100
