# Inference API
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082

# Workers
default_workers_per_model=2

# Batching
batch_size=16
max_batch_delay=50

# GPU
number_of_gpu=1
enable_envvars_config=true

# Logging
install_py_dep_per_model=true
enable_metrics_api=true
