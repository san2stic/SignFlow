services:
  backend:
    build: ./backend
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    ports:
      - "8000:8000"
    volumes:
      - ./backend/data:/app/data
      - ./backend/app:/app/app
    environment:
      - DATABASE_URL=postgresql+psycopg://${POSTGRES_USER:-signflow}:${POSTGRES_PASSWORD:-signflow}@db:5432/${POSTGRES_DB:-signflow}
      - REDIS_URL=redis://:${REDIS_PASSWORD:-signflow-dev}@redis:6379/0
      - SEARCH_BACKEND=elasticsearch
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - ELASTICSEARCH_INDEX=signflow-signs
      - ELASTICSEARCH_TIMEOUT_MS=2000
      - ELASTICSEARCH_REINDEX_ON_STARTUP=false
      - ELASTICSEARCH_FAIL_OPEN=true
      - ELASTICSEARCH_VERIFY_CERTS=false
      - MODEL_DIR=/app/data/models
      - VIDEO_DIR=/app/data/videos
      - EXPORT_DIR=/app/data/exports
      - CORS_ORIGINS=http://localhost:3000
      - TRUSTED_HOSTS=localhost,127.0.0.1,testserver,backend,frontend
      - TRAINING_USE_CELERY=true
      - USE_TORCHSERVE=false
      - TORCHSERVE_URL=http://torchserve:8080
      - TORCHSERVE_TIMEOUT_MS=2000
      - CANARY_PERCENTAGE=0
      - CANARY_MODEL_ID=
      - SHADOW_MODE_ENABLED=false
      - SHADOW_MODEL_ID=
      - SHADOW_MIN_CONFIDENCE=0.6
      - INFERENCE_METRICS_ENABLED=true
      - DRIFT_DETECTION_ENABLED=true
      - MLFLOW_REGISTRY_ENABLED=false
      - MLFLOW_REGISTRY_MODEL_NAME=signflow-model
      - MLFLOW_REGISTRY_AUTO_PROMOTE_STAGING=true
    depends_on:
      - db
      - redis
      - elasticsearch

  frontend:
    build: ./frontend
    command: sh -c "npm install && npm run dev -- --host 0.0.0.0 --port 3000"
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - frontend_node_modules:/app/node_modules
    environment:
      - VITE_DEV_PROXY_TARGET=http://backend:8000

  db:
    image: postgres:16-alpine
    volumes:
      - pgdata:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-signflow}
      - POSTGRES_USER=${POSTGRES_USER:-signflow}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-signflow}
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    command: redis-server --save 60 1000 --appendonly yes --requirepass ${REDIS_PASSWORD:-signflow-dev}
    ports:
      - "6379:6379"

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.15.3
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ports:
      - "9200:9200"
    volumes:
      - esdata:/usr/share/elasticsearch/data

  celery_worker:
    build: ./backend
    command: celery -A app.celery_app worker -l info -Q training
    volumes:
      - ./backend/data:/app/data
      - ./backend/app:/app/app
    environment:
      - DATABASE_URL=postgresql+psycopg://${POSTGRES_USER:-signflow}:${POSTGRES_PASSWORD:-signflow}@db:5432/${POSTGRES_DB:-signflow}
      - REDIS_URL=redis://:${REDIS_PASSWORD:-signflow-dev}@redis:6379/0
      - SEARCH_BACKEND=elasticsearch
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - ELASTICSEARCH_INDEX=signflow-signs
      - ELASTICSEARCH_TIMEOUT_MS=2000
      - ELASTICSEARCH_REINDEX_ON_STARTUP=false
      - ELASTICSEARCH_FAIL_OPEN=true
      - ELASTICSEARCH_VERIFY_CERTS=false
      - MODEL_DIR=/app/data/models
      - VIDEO_DIR=/app/data/videos
      - EXPORT_DIR=/app/data/exports
      - TRAINING_USE_CELERY=true
    depends_on:
      - redis
      - db
      - elasticsearch

  mlflow:
    image: python:3.11-slim
    command: >
      sh -c "pip install mlflow>=2.10.0 &&
             mlflow ui --host 0.0.0.0 --port 5001 --backend-store-uri file:///mlflow/mlruns"
    ports:
      - "5001:5001"
    volumes:
      - ./backend/data/models/mlruns:/mlflow/mlruns
    environment:
      - MLFLOW_TRACKING_URI=file:///mlflow/mlruns

  torchserve:
    build:
      context: ./backend
      dockerfile: Dockerfile.torchserve
    # platform: auto-detected (supprimé pour compatibilité multi-arch)
    container_name: signflow_torchserve
    ports:
      - "8080:8080"  # Inference API
      - "8081:8081"  # Management API
      - "8082:8082"  # Metrics API (Prometheus)
    volumes:
      - ./backend/torchserve/model-store:/home/model-server/model-store
      - ./backend/torchserve/config:/home/model-server/config
    environment:
      # Device detection automatique dans start.sh
      - TS_NUMBER_OF_GPU=${TS_NUMBER_OF_GPU:-0}  # 0=auto-detect
      - TS_INSTALL_PY_DEP_PER_MODEL=true
      - TORCH_DEVICE=${TORCH_DEVICE:-auto}  # auto, cpu, mps, cuda
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          memory: 2G

volumes:
  pgdata:
  esdata:
  frontend_node_modules:
