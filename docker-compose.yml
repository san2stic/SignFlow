services:
  backend:
    build: ./backend
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    ports:
      - "8000:8000"
    volumes:
      - ./backend/data:/app/data
      - ./backend/app:/app/app
    environment:
      - DATABASE_URL=postgresql+psycopg://${POSTGRES_USER:-signflow}:${POSTGRES_PASSWORD:-signflow}@db:5432/${POSTGRES_DB:-signflow}
      - REDIS_URL=redis://:${REDIS_PASSWORD:-signflow-dev}@redis:6379/0
      - MODEL_DIR=/app/data/models
      - VIDEO_DIR=/app/data/videos
      - EXPORT_DIR=/app/data/exports
      - CORS_ORIGINS=http://localhost:3000
      - TRUSTED_HOSTS=localhost,127.0.0.1,testserver,backend,frontend
      - TRAINING_USE_CELERY=true
    depends_on:
      - db
      - redis

  frontend:
    build: ./frontend
    command: sh -c "npm install && npm run dev -- --host 0.0.0.0 --port 3000"
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - VITE_DEV_PROXY_TARGET=http://backend:8000

  db:
    image: postgres:16-alpine
    volumes:
      - pgdata:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-signflow}
      - POSTGRES_USER=${POSTGRES_USER:-signflow}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-signflow}
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    command: redis-server --save 60 1000 --appendonly yes --requirepass ${REDIS_PASSWORD:-signflow-dev}
    ports:
      - "6379:6379"

  celery_worker:
    build: ./backend
    command: celery -A app.celery_app worker -l info -Q training
    volumes:
      - ./backend/data:/app/data
      - ./backend/app:/app/app
    environment:
      - DATABASE_URL=postgresql+psycopg://${POSTGRES_USER:-signflow}:${POSTGRES_PASSWORD:-signflow}@db:5432/${POSTGRES_DB:-signflow}
      - REDIS_URL=redis://:${REDIS_PASSWORD:-signflow-dev}@redis:6379/0
      - MODEL_DIR=/app/data/models
      - VIDEO_DIR=/app/data/videos
      - EXPORT_DIR=/app/data/exports
      - TRAINING_USE_CELERY=true
    depends_on:
      - redis
      - db

  mlflow:
    image: python:3.11-slim
    command: >
      sh -c "pip install mlflow>=2.10.0 &&
             mlflow ui --host 0.0.0.0 --port 5000 --backend-store-uri file:///mlflow/mlruns"
    ports:
      - "5000:5000"
    volumes:
      - ./backend/data/models/mlruns:/mlflow/mlruns
    environment:
      - MLFLOW_TRACKING_URI=file:///mlflow/mlruns

  torchserve:
    image: pytorch/torchserve:latest-gpu
    container_name: signflow_torchserve
    ports:
      - "8080:8080"  # Inference API
      - "8081:8081"  # Management API
      - "8082:8082"  # Metrics API (Prometheus)
    volumes:
      - ./backend/torchserve/model-store:/home/model-server/model-store
      - ./backend/torchserve/config:/home/model-server/config
    environment:
      - TS_NUMBER_OF_GPU=1
      - TS_INSTALL_PY_DEP_PER_MODEL=true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: 1

volumes:
  pgdata:
